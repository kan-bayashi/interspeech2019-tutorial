{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/espnet/interspeech2019-tutorial/blob/master/notebooks/interspeech2019_tts/interspeech2019_tts.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wA10eXyHIv8X",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# [T6] Advanced methods for neural end-to-end speech processing - unification, integration, and implementation -\n",
    "\n",
    "### Part4 : Building End-to-End TTS System\n",
    "\n",
    "Speaker: [**Tomoki Hayashi**](https://github.com/kan-bayashi)\n",
    "\n",
    "Department of informatics, Nagoya University  \n",
    "Human Dataware Lab. Co., Ltd.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Good afternoon, everyone.  \n",
    "This is Tomoki Hayashi, doctroral researcher @ Nagoya University.  \n",
    "From here, I will introduce the demonstration of the development of E2E-TTS system in ESPnet.  \n",
    "I will use google colaboratory for this hands-on.  \n",
    "So please access the tutorial material page and open the TTS hands on in your browser."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Google colaboratory\n",
    "\n",
    "**OUR HANDS-ON NOTEBOOK URL: https://bit.ly/2kz7wGD**\n",
    "\n",
    "- Online Jupyter notebook environment\n",
    "    - Can run python codes\n",
    "    - Can also run linux command with ! mark\n",
    "    - Can use a signal GPU (K80)\n",
    "- What you need to use\n",
    "    - Internet connection\n",
    "    - Google account\n",
    "    - Chrome browser (recommended)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Maybe most of you know the google colaboratory, here I briefly explain it.  \n",
    "Google colaboratory is the online jupyter notebook environment.  \n",
    "We can run the python code and linux command with exclamation mark in front of commands.  \n",
    "What you need to use is internet connection, google account, and chrome browser.  \n",
    "Maybe you can use in other browser, but we do not check so recommend to use chrome."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Usage of Google colaboratory\n",
    "\n",
    "<div align=center>\n",
    "    <img src=figs/colab_usage.png width=80%>\n",
    "</div>\n",
    "\n",
    "- Do not close the browser\n",
    "- Do not sleep your laptop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example of the commands\n",
    "print(\"hello, world.\")\n",
    "!echo \"hello, world\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "This is the usage of google colaboratory.  \n",
    "Basically it is the same as jupyter notebook.  \n",
    "You can run the cell by clicking this bottun or ctrl+enter.  \n",
    "Also you can add new code or text cell by clicking these bottons."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## TOC\n",
    "\n",
    "0. Installation\n",
    "1. Introduction of ESPnet TTS\n",
    "2. Demonstration of the ESPnet TTS recipe\n",
    "3. Demonstration of the use of TTS pretrained models\n",
    "4. Demonstration of the use of ASR pretrained models\n",
    "5. Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "This is the table of contents of my presentation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8xWr9FADn0Rf",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 0. Installation\n",
    " \n",
    "It takes around 3 minutes. Please keep waiting for a while.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "J6k0hPgzjxA5"
   },
   "outputs": [],
   "source": [
    "# OS setup\n",
    "!cat /etc/os-release\n",
    "!apt-get install -qq bc tree sox\n",
    "\n",
    "# espnet setup\n",
    "!git clone --depth 5 https://github.com/espnet/espnet\n",
    "!pip install -q torch==1.1\n",
    "!cd espnet; pip install -q -e .\n",
    "\n",
    "# download pre-compiled warp-ctc and kaldi tools\n",
    "!espnet/utils/download_from_google_drive.sh \\\n",
    "    \"https://drive.google.com/open?id=13Y4tSygc8WtqzvAVGK_vRV9GlV7TRC0w\" espnet/tools tar.gz > /dev/null\n",
    "!cd espnet/tools/warp-ctc/pytorch_binding && \\\n",
    "    pip install -U dist/warpctc_pytorch-0.1.1-cp36-cp36m-linux_x86_64.whl\n",
    "\n",
    "# make dummy activate\n",
    "!mkdir -p espnet/tools/venv/bin && touch espnet/tools/venv/bin/activate\n",
    "!echo \"setup done.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "First, installation.  \n",
    "Please run the cell and keep waiting for a while.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 1. Introduction of ESPnet TTS\n",
    "\n",
    "- Follow the [Kaldi](https://github.com/kaldi-asr/kaldi) style recipe\n",
    "- Support three E2E-TTS models and their variants\n",
    "- Support four corpus including English, Japanese, Italy, Spanish, and Germany\n",
    "- Support pretrained WaveNet-vocoder (Softmax and MoL version)\n",
    "\n",
    "Samples are available in https://espnet.github.io/espnet-tts-sample/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "During the installation, I will introduce the ESPnet TTS systems.  \n",
    "TTS recipes also follow the kaldi-style recipe, the most of preprocessing steps are the exactly same as the ASR.  \n",
    "Currently we support three E2E-TTS models and their variants.  \n",
    "We support four corpus including English, Japanese, Italy, Spanish and Germany.  \n",
    "Also, we support pretrained WaveNet vocoder. So you can generate higher quality speech.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Supported E2E-TTS models\n",
    "\n",
    "- [**Tacotron 2**](https://arxiv.org/abs/1712.05884): Standard Tacontron 2\n",
    "- [**Multi-speaker Tacotron2**](https://arxiv.org/pdf/1806.04558.pdf): Pretrained speaker embedding (X-vector) + Tacotron 2\n",
    "- [**Transformer**](https://arxiv.org/pdf/1809.08895.pdf): TTS-Transformer\n",
    "- [**Multi-speaker Transformer**](): Pretrained speaker embedding (X-vector) + TTS-Transformer\n",
    "- [**FastSpeech**](https://arxiv.org/pdf/1905.09263.pdf): Feed-forward TTS-Transformer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "We support following E2E-TTS models.\n",
    "Tacotron2, Multi-speaker Tacotron2 using pretrained speaker embedding, Transformer, Multi-speaker Transformer with pretrained speaker embedding, and FastSpeech also known as feed-forward Transformer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Other remarkable functions\n",
    "\n",
    "- [**CBHG** (Convolutional Bank Highway network Gated recurrent unit)](https://arxiv.org/pdf/1703.10135.pdf): Network to convert Mel-filter bank to linear spectrogram\n",
    "- [**Forward attention**](https://arxiv.org/pdf/1807.06736.pdf): Attention mechanism with causal regularization\n",
    "- [**Guided attention loss**](https://arxiv.org/pdf/1710.08969.pdf): Loss function to force attention to be diagonal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "As other remarkable functions, we support CBHG convolutional bank highway network gated recurrent unit network, which convert mel-filter bank to linear spectrogram, and forward attention, which has a causal regularization, and guided attention loss, which force attention to be diagonal.  \n",
    "During the training, we can combine these function to train E2E-TTS models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Supported corpora\n",
    "\n",
    "- [`egs/jsut/tts1`](https://sites.google.com/site/shinnosuketakamichi/publication/jsut): Japanese single female speaker. (48 kHz, ~10 hours)\n",
    "- [`egs/libritts/tts1`](http://www.openslr.org/60/): English multi speakers (24 kHz, ~500 hours).\n",
    "- [`egs/ljspeech/tts1`](https://keithito.com/LJ-Speech-Dataset/): English single female speaker (22.05 kHz, ~24 hours).\n",
    "- [`egs/m_ailabs/tts1`](https://www.caito.de/2019/01/the-m-ailabs-speech-dataset/): Various language speakers (16 kHz, 16~48 hours)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Here is the list of supported corpus, and recipes.  \n",
    "JSUT, LibriTTS, LJspeech and M ailabs speech dataset.  \n",
    "You can build various language TTS model through the ESPnet!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QhW6ey37Kj7y",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 2. Demonstration of ESPnet-TTS recipes\n",
    " \n",
    "Here use the recipe `egs/an4/tts1` as an example.  \n",
    "\n",
    "Unfortunately, `egs/an4/tts1` is too small to train,\n",
    "but the flow itself is the same as the other recipes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Maybe you finished the installation, right?  \n",
    "Let us move on the demonstration of ESPnet TTS recipe.  \n",
    "Here we use the recipe `egs/an4/tts1`, which is a TTS version of `egs/an4/asr1`.  \n",
    "An4 is provided by CMU and it is free and suitable for demonstration.  \n",
    "Unfortunately, `egs/an4/tts1` is too small to train, but the flow itself is the same as the other recipes.  \n",
    "So you can understand the basic flow of TTS recipes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kK-qFOFupi-I",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Always we organize each recipe placed in `egs/xxx/tts1` in the same manner:\n",
    "\n",
    "- `run.sh`: Main script of the recipe.\n",
    "- `cmd.sh`: Command configuration script to control how-to-run each job.\n",
    "- `path.sh`: Path configuration script. Basically, we do not need to touch.\n",
    "- `conf/`: Directory containing configuration files e.g.g.\n",
    "- `local/`: Directory containing the recipe-specific scripts e.g. data preparation.\n",
    "- `steps/` and `utils/`: Directory containing kaldi tools."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "We alway organize each recipe placed in `egs/xxx/tts1` in the same manner.  \n",
    "Let us move on the recipe directory and check the file. please run the following cell.  \n",
    "Each recipe contains the main script run.sh, command configuration script cmd.sh, path configuration script path.sh.  \n",
    "Also there are some directories, conf is the directory including configuration files, local is the directory including the recipe-specific scripts such as data preparation, and steps and utils the directories including kaldi tools.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 295
    },
    "colab_type": "code",
    "id": "X6T2rNlSpVMV",
    "outputId": "2cc5b211-b004-43b8-afae-d15672464f90"
   },
   "outputs": [],
   "source": [
    "# move on the recipe directory\n",
    "import os\n",
    "os.chdir(\"espnet/egs/an4/tts1\")\n",
    "\n",
    "# check files\n",
    "!tree -L 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aGX_y4RMpqIK",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Main script `run.sh` consists of several stages:\n",
    "\n",
    "<div align=center>\n",
    "    <img src=figs/tts_brief_overview.png width=75%>\n",
    "</div>\n",
    "\n",
    "- **stage -1**: Download data if the data is available online.\n",
    "- **stage 0**: Prepare data to make kaldi-stype data directory.\n",
    "- **stage 1**: Extract feature vector, calculate statistics, and normalize.\n",
    "- **stage 2**: Prepare a dictionary and make json files for training.\n",
    "- **stage 3**: Train the E2E-TTS network.\n",
    "- **stage 4**: Decode mel-spectrogram using the trained network.\n",
    "- **stage 5**: Generate a waveform using Griffin-Lim.\n",
    "\n",
    "From **stage -1 to 2 are the same as the ASR** recipe."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "The main script run.sh consists of several stages from -1 to 5, in total 6 stages.  \n",
    "Data download, kaldi-style data preparation, feature extraction, data conversion for json format, Training, decoding and synthesis.  \n",
    "Stage 0 and 1 are performed with kaldi and librosa, and stage 3 and 4 are performed by pytorch.  \n",
    "From stage -1 to stage 2 are the same as ASR.  \n",
    "We have the unified flow in both TTS and ASR."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Detail overview\n",
    "<div align=center>\n",
    "    <img src=figs/tts_overview.png width=90%>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "This figure shows the detail flow of each recipe.  \n",
    "I will introduce each stage step-by-step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "l6Zhc4iKLKeC",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Stage -1: Data download\n",
    "\n",
    "This stage downloads corpus if it is available online.\n",
    "\n",
    "<div align=center>\n",
    "    <img src=figs/tts_stage-1.png width=80%>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "First, stage -1 data download.  \n",
    "This stage downloads corpus from cloud if the data is available online."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 69
    },
    "colab_type": "code",
    "id": "8CNFWXETAeYC",
    "outputId": "e6559ffa-c534-4cfe-9295-f5a82924f8d7",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# run stage -1 and then stop\n",
    "!./run.sh --stage -1 --stop_stage -1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Please run the cell. And keep waiting for a while."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lzqGzOqpw741",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "`downloads` directory is created, which containing downloaded an4 dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 312
    },
    "colab_type": "code",
    "id": "Y3FTgWO7w0Rf",
    "outputId": "250de278-54c6-4e28-fd85-64d8f84c2942",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "!tree -L 2 downloads"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Downloaded files are save in `downloads` directory.  \n",
    "Let me check the files.  \n",
    "You can see the corpus is stored in the directory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wa1OkxN3xSRx",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Stage 0: Data preparation\n",
    "\n",
    "This stage prepares kaldi-style data directories.\n",
    "\n",
    "<div align=center>\n",
    "    <img src=figs/tts_stage0.png width=80%>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Next, stage 0 data preparation.  \n",
    "This stage prepares the kaldi-style data directories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "ozLaRF1LA3Aq",
    "outputId": "5ac935b7-2fc7-415c-ccff-1b45e308adab",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# run stage 0 and then stop\n",
    "!./run.sh --stage 0 --stop_stage 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Please run the cell."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bF6wu5n3x9gD",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Two kaldi-style data directories are created:  \n",
    "- `data/train`: data directory of training set\n",
    "- `data/test`: data directory of evaluation set  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 156
    },
    "colab_type": "code",
    "id": "5Ch0fonjxep5",
    "outputId": "48bc5bd7-80e5-4794-9925-ae67c3fc51cb",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "!tree -L 2 data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Through this stage, 2 kaldi-style data directories are created under the `data` directory.  \n",
    "Let me check the files."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EwPydMzvxbcA",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "`wav.scp`: \n",
    "- Each line has `<utt_id> <wavfile_path or command pipe>`\n",
    "- `<utt_id>` must be unique\n",
    "\n",
    "`text`:\n",
    "- Each line has `<utt_id> <transcription>`\n",
    "- Assume that `<transcription>` is cleaned\n",
    "\n",
    "`utt2spk`:\n",
    "- Each line has `<utt_id> <speaker_id>`\n",
    "\n",
    "`spk2utt`:\n",
    "- Each line has `<speaker_id> <utt_id> ... <utt_id> `\n",
    "- Can be automatically created from `utt2spk` \n",
    "\n",
    "In the ESPnet, speaker information is **not used for any processing.**   \n",
    "Therefore, **`utt2spk` and `spk2utt` can be a dummy.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 347
    },
    "colab_type": "code",
    "id": "S5PJH9zxyqNg",
    "outputId": "88f99c71-9309-494c-ec36-82e3fd29c93d"
   },
   "outputs": [],
   "source": [
    "!head -n 3 data/train/*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Maybe most of you are the specialist of the kaldi, you know what these files are.  \n",
    "So here briefly check the files. please run the cell.\n",
    "These fundamental files are the same as kaldi but there is an important point.  \n",
    "That is, in the ESPnet, any speaker information is not used for any processing.  \n",
    "Therefore utt2spk and spk2utt can be a dummy.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XEgfec6u1KWA",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Stage 1: Feature extration\n",
    "\n",
    "This stage performs feature extraction, statistics calculation and normalization.\n",
    "\n",
    "<div align=center>\n",
    "    <img src=figs/tts_stage1.png width=80%>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Next, stage 1 feature extration.  \n",
    "This stage performes feature extraction, calculatin of statistics of feature vector, and feature normalization using the claculated statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# hyperparameters related to stage 1\n",
    "!head -n 28 run.sh | tail -n 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Let me check the hyperparameters related to stage 1.  \n",
    "In the TTS, we use mel-filterbank as speech features and they are extracted by librosa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 364
    },
    "colab_type": "code",
    "id": "znHW0IwbBX0o",
    "outputId": "957f9dce-c181-433a-812b-3c79d2933653",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# run stage 1 with default settings\n",
    "!./run.sh --stage 1 --stop_stage 1 --nj 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Please run the stage 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "e9OS6JIp474Z",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Raw filterbanks are saved in `fbank/` directory with `ark/scp` format. \n",
    "\n",
    "- `.ark`: binary file of feature vector\n",
    "- `.scp`: list of the correspondance b/w `<utt_id>` and `<path_in_ark>`.  \n",
    "\n",
    "Since feature extraction can be performed for split small sets in parallel, raw_fbank is split into `raw_fbank_*.{1..4}.{scp,ark}`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!tree -L 2 fbank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 69
    },
    "colab_type": "code",
    "id": "tuPfH0ly46Ko",
    "outputId": "76de2cad-96b9-4781-afd3-7509960de58c"
   },
   "outputs": [],
   "source": [
    "!head -n 3 fbank/raw_fbank_train.1.scp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Through the stage 1, extracted raw filterbanks are saved in `fbank` directory with ark/scp format.  \n",
    "Let me check the files.\n",
    ".ark is binary file containing the feature vector and .scp is the list of the correspondance between utt_id and path_in_ark.  \n",
    "Since the feature extraction is performed in parallel, ark/scp files are split into several pieces."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oj1bx25q5bl_",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "These files can be loaded in python via a great tool **kaldiio** as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 545
    },
    "colab_type": "code",
    "id": "3K0uiQgB5aYK",
    "outputId": "d7797bdd-820b-45e4-ca91-ea821d5e7b36"
   },
   "outputs": [],
   "source": [
    "import kaldiio\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# load scp file\n",
    "scp_dict = kaldiio.load_scp(\"fbank/raw_fbank_train.1.scp\")\n",
    "for key in scp_dict:\n",
    "    plt.imshow(scp_dict[key].T[::-1])\n",
    "    plt.title(key)\n",
    "    plt.colorbar()\n",
    "    plt.show()\n",
    "    break\n",
    "    \n",
    "# load ark file\n",
    "ark_generator = kaldiio.load_ark(\"fbank/raw_fbank_train.1.ark\")\n",
    "for key, array in ark_generator:\n",
    "    plt.imshow(array.T[::-1])\n",
    "    plt.title(key)\n",
    "    plt.colorbar()\n",
    "    plt.show()\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "These ark/scp format files can be loaded as numpy.array, thanks to the great tool Kaldiio created by Mr. Kamo.  \n",
    "Please run the cell.  scp can be loaded as dict and ark can be loaded as a generator."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-Au70T7i8ctz",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Some files are added in `data/train`:\n",
    "- `feats.scp`: concatenated scp file of `fbank/raw_fbank_train.{1..4}.scp`.  \n",
    "- `utt2num_frames`: Each line has `<utt_id> <number_of_frames>` ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 191
    },
    "colab_type": "code",
    "id": "ySykrx407Tek",
    "outputId": "a0829cb1-f291-4d51-cd17-e7da04ec3d53"
   },
   "outputs": [],
   "source": [
    "!tree data/train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!head -n 3 data/train/*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Through feature extraction, some files are added in `data` directories.  \n",
    "Let me check these files.  \n",
    "feats.scp is the concatenated scp file of scp files in fbank directory.  \n",
    "utt2num_frames has the information of number of frames of each utterance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qt4s7lkl9miZ",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "`data/train/` directory is split into two directories:\n",
    "- `data/train_nodev/`: data directory for training\n",
    "- `data/train_dev/`: data directory for validation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 121
    },
    "colab_type": "code",
    "id": "Yq6WNznw9P4z",
    "outputId": "be2c96cc-c9fb-4068-b1d0-3367a15cbb8f"
   },
   "outputs": [],
   "source": [
    "!tree data/train_*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "And then data/train directory is split into two directories.  \n",
    "data/train_nodev and data/train_dev. These are for training and validation respectively.  \n",
    "Let me check the files. You can see both directories has the same files."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<div align=center>\n",
    "    <img src=figs/tts_stage1.png width=80%>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "So far, I explained this part.  \n",
    "Next is statistics calculatin and normalization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Fw_KBzQCBN0E",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "`cmvn.ark` is saved in `data/train_nodev`, which is the statistics file.  \n",
    "(cepstral mean variance normalization: `cmvn`)  \n",
    "This file also can be loaded in python via kaldiio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!tree data/train_nodev"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "statistics file is saved as `cmvn.ark` in data/train_nodev directory.  \n",
    "Let me check the file.  \n",
    "This file can also be loaded as numpy.array via kaldiio."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pTCG1CPRDN5H",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Normalized features for train, dev, and eval sets are dumped in\n",
    "- `dump/{train_nodev,train_dev,test}/*.{ark,scp}`.  \n",
    "\n",
    "These ark and scp can be loaded as the same as the above procedure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 260
    },
    "colab_type": "code",
    "id": "JazwdIVbDNFs",
    "outputId": "0ac7485e-7ea7-4519-a7e4-347d041e9389"
   },
   "outputs": [],
   "source": [
    "!tree dump/*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "And finally, normalized features for training, validation, and evaluation set are saved in `dump` directory with the ark/scp format.\n",
    "Let me check the files.  \n",
    "We also support online normalization during training, but basically to reduce the cpu calculation, we normalize features and then dump them like this.  \n",
    "Also, we can use kaldi pipe processing for normalization, but in espnet, to make it more python friendly, we explicitly dump the feature vector."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eYtmQCDdDk0d",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Stage 2: Dictionary and json preparation\n",
    "\n",
    "This stage creates char dict and integrate files into a single json file.\n",
    "\n",
    "<div align=center>\n",
    "    <img src=figs/tts_stage2.png width=80%>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Next stage 2 dictionary and json data preparation.  \n",
    "This stage creates char dict and integrate kaldi-style directories into a single json file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 173
    },
    "colab_type": "code",
    "id": "fgWDdveUBctQ",
    "outputId": "1f32337d-5e62-4462-d908-224b277ee647",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# run stage 2 and then stop\n",
    "!./run.sh --stage 2 --stop_stage 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Please run the stage 2. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IB-eU-W1FNLk",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- Dictionary file is created in `data/lang_1char/`.  \n",
    "- Dictionary file consists of `<token>` `<token index>`.  \n",
    "    - `<token index>` starts from 1 because 0 is used as padding index.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 521
    },
    "colab_type": "code",
    "id": "q8S74SjdFXcF",
    "outputId": "02f0fd30-a05f-4804-b6a8-5938782269d6"
   },
   "outputs": [],
   "source": [
    "!tree data/lang_1char"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat data/lang_1char/train_nodev_units.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Through this stage, dictionary file is created in data/lang_1char/.  \n",
    "Let me check the file.  \n",
    "The dictionary file consists of token and token index.  \n",
    "The index starts from 1 because 0 is used as padding index in TTS.  \n",
    "\n",
    "But in the case of ASR, 0 is used for blank symbol of CTC, please be careful."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "b-QO0MLGGaBO",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Three json files are created for train, dev, and eval sets as \n",
    "- `dump/{train_nodev,train_dev,test}/data.json`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "BrYKpYVUGh6K",
    "outputId": "1f168c10-6407-411e-b617-eafc5b1dae24"
   },
   "outputs": [],
   "source": [
    "!tree dump -L 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "And json file for training, validation, and evaluaton set are created under the `dump` directory.  \n",
    "Let me check the files. You can see the `data.json` in each directory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CoRMOHDnG1Xk",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Each json file contains all of the information in the data directory.\n",
    "\n",
    "- `shape`: Shape of the input or output sequence.\n",
    "- `text`: Original transcription.\n",
    "- `token`: Token sequence of the transcription.\n",
    "- `tokenid` Token id sequence converted with `dict` of the transcription"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 486
    },
    "colab_type": "code",
    "id": "O_My36NFHAVI",
    "outputId": "02642af2-6ae1-4c64-ab74-3b9ee2ada422"
   },
   "outputs": [],
   "source": [
    "!head -n 27 dump/train_nodev/data.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Let me check the content of json file. Please run the cell.  \n",
    "Each json contains allfo the information in the kaldi-style data directory.  \n",
    "for example, shape, text, token, tokenid, spk and so on.  \n",
    "Some of you noticed that input and output is reversed in terms of TTS.   \n",
    "This is because we use totally the same json file for TTS as ASR.  \n",
    "\n",
    "I'm sorry I kept you waiting, now ready to start training!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "O88CpXHtHnZA"
   },
   "source": [
    "Now ready to start training!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QUkLphZbIw51",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Stage 3: Network training\n",
    "\n",
    "This stage trains E2E-TTS network.\n",
    "\n",
    "<div align=center>\n",
    "    <img src=figs/tts_stage3.png width=80%>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Next, Stage 3 E2E-TTS training.  \n",
    "This stage trains E2E-TTS network using prepared json files."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Training setting can be specified by `train_config`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 816
    },
    "colab_type": "code",
    "id": "b-f5K2bYOaVY",
    "outputId": "22c2ec06-6a8a-4440-8ab3-e3eb877fdb68"
   },
   "outputs": [],
   "source": [
    "# check hyperparmeters in run.sh\n",
    "!head -n 31 run.sh | tail -n 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training configuration is written in `.yaml` format file.  \n",
    "Let us check the default configuration `conf/train_pytroch_tacotron2.yaml`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "The network configuration can be specified by `--train_config` option.  \n",
    "And the configuration file is written in yaml format.  \n",
    "Let me check the default configuration conf/train_pytorch_tacotron2.yaml."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "!cat conf/train_pytorch_tacotron2.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "There are several hyperparameters.  \n",
    "The default values are based on the official paper.  \n",
    "But these values are a little bit big for demonstration.  \n",
    "So let us change the hyperparameters by editing yaml file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6_QmCAS1OlD2",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Let's change the hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 816
    },
    "colab_type": "code",
    "id": "f2Q9sYVxOjzm",
    "outputId": "e32d1ff5-dd58-41b0-8475-0eeb8ca65f16"
   },
   "outputs": [],
   "source": [
    "# load configuration yaml\n",
    "import yaml\n",
    "with open(\"conf/train_pytorch_tacotron2.yaml\") as f:\n",
    "    params = yaml.load(f, Loader=yaml.Loader)\n",
    "\n",
    "# change hyperparameters by yourself!\n",
    "params.update({\n",
    "    \"embed-dim\": 16,\n",
    "    \"elayers\": 1, \n",
    "    \"eunits\": 16,\n",
    "    \"econv-layers\": 1,\n",
    "    \"econv-chans\": 16,\n",
    "    \"econv-filts\": 5,\n",
    "    \"dlayers\": 1,\n",
    "    \"dunits\": 16,\n",
    "    \"prenet-layers\": 1,\n",
    "    \"prenet-units\": 16,\n",
    "    \"postnet-layers\": 1,\n",
    "    \"postnet-chans\": 16,\n",
    "    \"postnet-filts\": 5,\n",
    "    \"adim\": 16,\n",
    "    \"aconv-chans\": 16,\n",
    "    \"aconv-filts\": 5,\n",
    "    \"reduction-factor\": 5,\n",
    "    \"batch-size\": 128,\n",
    "    \"epochs\": 5,\n",
    "    \"report-interval-iters\": 10,\n",
    "})\n",
    "\n",
    "# save\n",
    "with open(\"conf/train_pytorch_tacotron2_mini.yaml\", \"w\") as f:\n",
    "    yaml.dump(params, f, Dumper=yaml.Dumper)\n",
    "\n",
    "# check modified version\n",
    "!cat conf/train_pytorch_tacotron2_mini.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Here we show the how to edit in python but of course you can edit it by your favorite editor such as vim.  \n",
    "We save modified yaml file as train_pytorch_tacotron2_mini.yaml."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Also, we provide `transformer` and `fastspeech` configs.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "!cat ../../ljspeech/tts1/conf/tuning/train_pytorch_transformer.v1.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "!cat ../../ljspeech/tts1/conf/tuning/train_fastspeech.v2.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "We can easily switch the model to be trained by only changing `--train_config`.  \n",
    "(NOTE: FastSpeech needs a teacher model, pretrained Transformer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Also we provide the transformer and fastspeech configs. \n",
    "Let me check them.\n",
    "But before the explanation of these configuration, let us run the training using the modified config.  \n",
    "Please move on the next cell.\n",
    "\n",
    "(After return)  \n",
    "During the training, I will explain the transformer and fastspeech configs.  \n",
    "Please go back to the previous cell.  \n",
    "These are the transformer and fastspeech configs.  \n",
    "We can easily switch the model to train by only changing `--train_config`.\n",
    "Compared to the tacotron2, Trasnformer is more difficult to train due to the interesting attention weight behavior and a large batchsize requirement.  \n",
    "But it leads the training of FastSpeech which makes it possible to do super fast generation.\n",
    "I can show you the behavior in the latter pretrained model demonstration."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MlkD_x4wRD80",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Let's train the network.  \n",
    "You can specify the config file via `--train_config` option.  \n",
    "It takes several minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 69
    },
    "colab_type": "code",
    "id": "cgdY6vp0Bhy4",
    "outputId": "73a6fc3a-5e44-4dd7-c229-42037276ba44",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# use modified configuration file as train config\n",
    "!./run.sh --stage 3 --stop_stage 3 --train_config conf/train_pytorch_tacotron2_mini.yaml --verbose 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "OK. let us train the network with modified configuration!  \n",
    "You can specify the config file vie --train_config option like this.  \n",
    "It takes several minutes. Please keep waiting for a while."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HsN8qD1gKnTc",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "You can see the training log in `exp/train_*/train.log`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat exp/train_nodev_pytorch_train_pytorch_tacotron2_mini/train.log"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "OK. Maybe the training was finished, right?\n",
    "Let me check the training log."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wy0rXqBNTAtk",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The models are saved in `exp/train_*/results/` directory.\n",
    "\n",
    "- `exp/train_*/results/model.loss.best`: contains only the model parameters.  \n",
    "- `exp/train_*/results/snapshot.ep.*`: contains the model parameters, optimizer states, and iterator states. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 208
    },
    "colab_type": "code",
    "id": "YRxIFjnWS_t9",
    "outputId": "5408227c-5118-4b6f-f36f-e6a72e3ddc58"
   },
   "outputs": [],
   "source": [
    "!tree -L 1 exp/train_nodev_pytorch_train_pytorch_tacotron2_mini/results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "The models are saved in exp/train_*/results directory.  \n",
    "model.*.best contain only the model parameters on the other hand snapshot contains the model parameters, optimizer states, and iterator states."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_vyKmHl8T5uj",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "`exp/train_*/results/*.png` are the figures of training curve.  \n",
    "Let us check them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "Dwee4a8NUOBg",
    "outputId": "ea2a6c3f-4296-4ab7-f918-2d378227aec8"
   },
   "outputs": [],
   "source": [
    "from IPython.display import Image, display_png\n",
    "print(\"all loss curve\")\n",
    "display_png(Image(\"exp/train_nodev_pytorch_train_pytorch_tacotron2_mini/results/all_loss.png\", width=500))\n",
    "print(\"l1 loss curve\")\n",
    "display_png(Image(\"exp/train_nodev_pytorch_train_pytorch_tacotron2_mini/results/l1_loss.png\", width=500))\n",
    "print(\"mse loss curve\")\n",
    "display_png(Image(\"exp/train_nodev_pytorch_train_pytorch_tacotron2_mini/results/mse_loss.png\", width=500))\n",
    "print(\"bce loss curve\")\n",
    "display_png(Image(\"exp/train_nodev_pytorch_train_pytorch_tacotron2_mini/results/bce_loss.png\", width=500))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Also, the training curve figures are saved in results directory.  \n",
    "Let me check the figures.  \n",
    "These figures are continuously updated during training, so you can monitor with these figures."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Ui57IrR4VCpv",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "`exp/train_*/results/att_ws/*.png` are the figures of attention weights in each epoch.  \n",
    "In the case of E2E-TTS, it is very important to check that they are diagonal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 514
    },
    "colab_type": "code",
    "id": "NyMW2tx0VKjC",
    "outputId": "d301ec6f-1a0a-4aa5-efb7-c69fd978d4ae"
   },
   "outputs": [],
   "source": [
    "print(\"Attention weights of initial epoch\")\n",
    "display_png(Image(\"exp/train_nodev_pytorch_train_pytorch_tacotron2_mini/results/att_ws/fash-cen1-b.ep.1.png\", width=500))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Also, the figures of attention weights for validation data are saved every epochs in results/att_ws directory.  \n",
    "Let me check the figure.  \n",
    "In the case of TTS, monitoring attention weight is very important to check the training going well.  \n",
    "In the case of this figure, attention weight is not diagonal, it means the model cannot generate any speech.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Example of a good diagonal attention weights:\n",
    "<div align=center>\n",
    "    <img src=figs/ex_attention_weights.png width=60%>\n",
    "</div>\n",
    "We should monitor whether the attention weight becomes like this figure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Here is the example of good attenion weights.  \n",
    "As you can see, attention weight is diagonal and causal.  \n",
    "We should monitor whether the attention weight becomes like this figure during training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JjkrYF-NeZoM",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Also, we support tensorboard.  \n",
    "You can see the training log through tensorboard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OqwnbGznmrgb"
   },
   "outputs": [],
   "source": [
    "# only available in colab\n",
    "%load_ext tensorboard\n",
    "%tensorboard --logdir tensorboard/train_nodev_pytorch_train_pytorch_tacotron2_mini/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Also we support tensorboard.  \n",
    "You can see training curve or attention weights explained on the above through tensorboard."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "88UvHDgGJSLX",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Stage 4: Network decoding\n",
    "\n",
    "This stage performs decoding with trained model.\n",
    "\n",
    "<div align=center>\n",
    "    <img src=figs/tts_stage4.png width=80%>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Then, next, Stage 4 E2E-TTS decoding.  \n",
    "This stage performs decoding using trained E2E-TTS model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Decoding parameters can be specified by `--decode_config`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!head -n 32 run.sh | tail -n 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decoding configuration in written in `.yaml` format file.  \n",
    "Let us check the default configuration `conf/decode.yaml`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat conf/decode.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Decoding parameters can be specified by `--decod_config`.  \n",
    "Let me check the default decoding yaml file `conf/decode.yaml`.  \n",
    "threshold is the threshold to stop the feature genearation and the others are for avoiding endless generation.  \n",
    "Contrast to ASR, TTS decoding parameters are not needed to tune carefully."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PjVP2QPXBmav",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# run stage 4 and then stop\n",
    "!./run.sh --stage 4 --stop_stage 4 --nj 8 --verbose 1 --train_config conf/train_pytorch_tacotron2_mini.yaml "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Please run the stage 4. It takes several seconds.\n",
    "\n",
    "(it takes time, be careful.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "I7-8ja9kcdDG",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Generated features are saved as `ark/scp` format.  \n",
    "Also figures of attention weights and stop probabilities are saved as `{att_ws/probs}/*.png`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8V_g_VbDckjT"
   },
   "outputs": [],
   "source": [
    "!tree -L 2 exp/train_nodev_pytorch_train_pytorch_tacotron2_mini/outputs_model.loss.best_decode/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Generated features are saved as ark/scp format.  \n",
    "Also attention weights and stop probabilities saved as figures.  \n",
    "Let me check the files.  \n",
    "By checking the attention weights, we can check the generation is succeeded, or repeating, or deletion.  \n",
    "Again, in the TTS, checking attention weights is very important."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9A74Jv8KL_FC",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Stage 5: Waveform synthesis\n",
    "\n",
    "This stage synthesizes waveform with Griffin-Lim.\n",
    "\n",
    "<div align=center>\n",
    "    <img src=figs/tts_stage5.png width=80%>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "This is the final stage, waveform synthesis.  \n",
    "This stage performed denormalization of generated features and then synthesizes waveform with Griffin-Lim."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wasoVKklc4eS",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# run stage 5 and then stop\n",
    "!./run.sh --stage 5 --stop_stage 5 --nj 8 \\\n",
    "    --griffin_lim_iters 8 \\\n",
    "    --train_config conf/train_pytorch_tacotron2_mini.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Please run stage 5."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "E7eoOkRYeIhN",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Generated wav files are saved in \n",
    "- `exp/train_nodev_pytorch_*/outputs_model.loss.best_decode_denorm/{train_dev,test}/wav/`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kp6LcFfih0hJ"
   },
   "outputs": [],
   "source": [
    "!tree -L 2 exp/train_nodev_pytorch_train_pytorch_tacotron2_mini/*_denorm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Generated waveforms are saved in here.  \n",
    "Let me check the files.  \n",
    "You can see generated wavefiles.  \n",
    "Now you finished building your own E2E-TTS model!  \n",
    "Unfortunately, this model cannnot generate a good speech.  \n",
    "So let us listen to the samples in demo HP."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "Now you finish building your own E2E-TTS model!\n",
    "\n",
    "But unfortunately, this model cannot generate a good speech.  \n",
    "Let us listen to the samples in demo HP to check the quality.  \n",
    "https://espnet.github.io/espnet-tts-sample/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 3. Demonstration of the use of TTS pretrained models\n",
    "\n",
    "We provide pretrained TTS models and these are easy to use with `espnet/utils/synth_wav.sh`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# move on directory\n",
    "os.chdir(\"../../librispeech/asr1\")\n",
    "!pwd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "OK. Let us move on the next demonstration.  \n",
    "That is the use of TTS pretrained model.  \n",
    "ESPnet provides some utilities tools to use pretrained model.  \n",
    "Here I will introduce the usage of them.  \n",
    "Please run the cell to move on the directory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Let us check the usage of `espnet/utils/synth_wav.sh`.  \n",
    "It will automatically downloads pretrained model from online, you do not need to prepare anything.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!../../../utils/synth_wav.sh --help"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "To use the TTS pretrained model, we provide `synth_wav.sh`.  \n",
    "Let me check the usage of it.  \n",
    "This script will automatically download specified pretrained model and then perform generation. \n",
    "So you do not need to prepare anything."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Let us generate your own text with pretrained models!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# generate your sentence!\n",
    "!rm -rf decode/example\n",
    "print(\"Please input your favorite sentence!\")\n",
    "text = input()\n",
    "text = text.upper()\n",
    "with open(\"example.txt\", \"w\") as f:\n",
    "    f.write(text)\n",
    "\n",
    "# you can change here to select the pretrained model\n",
    "# !../../../utils/synth_wav.sh --models ljspeech.fastspeech.v1 example.txt\n",
    "# !../../../utils/synth_wav.sh --models ljspeech.tacotron2.v3 example.txt\n",
    "!../../../utils/synth_wav.sh --models ljspeech.transformer.v1 example.txt\n",
    "\n",
    "# check generated audio\n",
    "from IPython.display import display, Audio, Image, display_png\n",
    "display(Audio(\"decode/example/wav/example.wav\"))\n",
    "!sox decode/example/wav/example.wav -n rate 22050 spectrogram\n",
    "display_png(Image(\"spectrogram.png\", width=750))\n",
    "\n",
    "# check attention and probs\n",
    "if os.path.exists(\"decode/example/outputs/att_ws/example_att_ws.png\"):\n",
    "    display_png(Image(\"decode/example/outputs/att_ws/example_att_ws.png\", width=1000))\n",
    "    display_png(Image(\"decode/example/outputs/probs/example_prob.png\", width=500))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Let us generate your sentence. Please run the cell.  \n",
    "At first we use pretrained fastspeech.  \n",
    "Let me check the sample.\n",
    "Next let me use tacotron2.  \n",
    "You can see the attention weights and stop probabilities.  \n",
    "The attention weights are diagonal, which is an evidence of successful generation.\n",
    "Finally, let us use Transformer.\n",
    "You can see many attention weigths.  \n",
    "Interestingly, only the some of the heads are diagonal in Transformer.  \n",
    "And the attention is not continuous."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Also you can try the wavenet vocoder, but it takes time to decode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate your sentence!\n",
    "!rm -rf decode/example_short\n",
    "print(\"Please input your favorite sentence!\")\n",
    "text = input()\n",
    "text = text.upper()\n",
    "with open(\"example_short.txt\", \"w\") as f:\n",
    "    f.write(text)\n",
    "    \n",
    "# extend stop_stage\n",
    "!../../../utils/synth_wav.sh --stop_stage 4 --models ljspeech.tacotron2.v3 example_short.txt\n",
    "\n",
    "# check generated audio\n",
    "display(Audio(\"decode/example_short/wav/example_short.wav\"))\n",
    "display(Audio(\"decode/example_short/wav_wnv/example_short_gen.wav\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "You also try the wavenet vocoder, but it takes time.  \n",
    "Let us generate very short sentence.  \n",
    "You can confirm that the naturalness improved significantly but the pronunciation itself is the same."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 3. Demonstration of the use of ASR pretrained models\n",
    "\n",
    "ESPnet also provides the `espnet/utils/recog_wav.sh` to use pretrained ASR models.  \n",
    "Let us recognize the generated speech!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "!../../../utils/recog_wav.sh --help"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "ESPnet also provides the utility `recong_wav.sh` to use pretrained ASR model.  \n",
    "Let us check the usage.  \n",
    "This script will also automatically download pretrained ASR model.  \n",
    "So you do not need to prepare anything."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# downsample to 16 kHz for ASR model\n",
    "!sox decode/example/wav/example.wav -b 16 decode/example/wav/example_16k.wav rate 16k\n",
    "\n",
    "# make decode config\n",
    "import yaml\n",
    "with open(\"conf/decode_sample.yaml\", \"w\") as f:\n",
    "    yaml.dump({\n",
    "        \"batchsize\": 0,\n",
    "        \"beam-size\": 5,\n",
    "        \"ctc-weight\": 0.4,\n",
    "        \"lm-weight\": 0.6,\n",
    "        \"maxlenratio\": 0.0,\n",
    "        \"minlenratio\": 0.0,\n",
    "        \"penalty\": 0.0,\n",
    "    }, f, Dumper=yaml.Dumper)\n",
    "\n",
    "# let's recognize generated speech\n",
    "!../../../utils/recog_wav.sh --models librispeech.transformer.v1 \\\n",
    "    --decode_config conf/decode_sample.yaml \\\n",
    "    decode/example/wav/example_16k.wav"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Here, let us recognize the generated speech.  \n",
    "Librispeech transformer models is big, it takes several seconds.  \n",
    "Yes, the model successfully recognized the generated speech."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Conclusion\n",
    "\n",
    "- Can build E2E-TTS models with unified-design recipe\n",
    "- Can try various models by just changing the yaml file\n",
    "\n",
    "Through ESPnet, you can build / use E2E-TTS and E2E-ASR in the same manner!\n",
    "\n",
    "Thank you for your attention!\n",
    "\n",
    "[*Go to the next notebook from here!*](https://colab.research.google.com/github/espnet/interspeech2019-tutorial/blob/master/notebooks/interspeech2019_asr/interspeech2019_asr.ipynb)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "celltoolbar": "Slideshow",
  "colab": {
   "collapsed_sections": [],
   "name": "interspeech2019_tts.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "rise": {
   "scroll": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
